{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import argparse\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import concurrent.futures\n",
    "from typing import Dict, List\n",
    "from src.runner import Runner\n",
    "import os\n",
    "from src.plotPR import PRCurves\n",
    "\n",
    "class InputSettings(object):\n",
    "    def __init__(self,\n",
    "            datadir, datasets, algorithms) -> None:\n",
    "        \n",
    "        self.datadir = datadir\n",
    "        self.datasets = datasets\n",
    "        self.algorithms = algorithms\n",
    "\n",
    "class OutputSettings(object):\n",
    "    '''\n",
    "    Structure for storing the names of directories that output should\n",
    "    be written to\n",
    "    '''\n",
    "\n",
    "    def __init__(self, base_dir: Path) -> None:\n",
    "        self.base_dir = base_dir\n",
    "\n",
    "        \n",
    "class Evaluation(object):\n",
    "    '''\n",
    "    The Evaluation object is created by parsing a user-provided configuration\n",
    "    file. Its methods provide for further processing its inputs into\n",
    "    a series of jobs to be run, as well as running these jobs.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "            input_settings: InputSettings,\n",
    "            output_settings: OutputSettings) -> None:\n",
    "\n",
    "        self.input_settings = input_settings\n",
    "        self.output_settings = output_settings\n",
    "        self.runners: Dict[int, Runner] = self.__create_runners()\n",
    "\n",
    "\n",
    "    def __create_runners(self) -> Dict[int, List[Runner]]:\n",
    "        '''\n",
    "        Instantiate the set of runners based on parameters provided via the\n",
    "        configuration file. Each runner is supplied an interactome, collection,\n",
    "        the set of algorithms to be run, and graphspace credentials, in\n",
    "        addition to the custom parameters each runner may or may not define.\n",
    "        '''\n",
    "        \n",
    "        runners: Dict[int, Runner] = defaultdict(list)\n",
    "        order = 0\n",
    "        for dataset in self.input_settings.datasets:\n",
    "            for runner in self.input_settings.algorithms:\n",
    "                data = {}\n",
    "                data['name'] = runner[0]\n",
    "                data['params'] = runner[1]\n",
    "                data['inputDir'] = Path.cwd().joinpath(self.input_settings.datadir.joinpath(dataset['name']))\n",
    "                print(data['inputDir'])\n",
    "                runners[order] = Runner(data)\n",
    "                order += 1            \n",
    "        return runners\n",
    "\n",
    "\n",
    "    def execute_runners(self, parallel=False, num_threads=1):\n",
    "        '''\n",
    "        Run each of the algorithms\n",
    "        '''\n",
    "\n",
    "        base_output_dir = self.output_settings.base_dir\n",
    "\n",
    "        batches =  self.runners.keys()\n",
    "\n",
    "        for batch in batches:\n",
    "            if parallel==True:\n",
    "                executor = concurrent.futures.ThreadPoolExecutor(max_workers=1)\n",
    "                futures = [executor.submit(runner.run, base_output_dir)\n",
    "                    for runner in self.runners[batch]]\n",
    "                \n",
    "                # https://stackoverflow.com/questions/35711160/detect-failed-tasks-in-concurrent-futures\n",
    "                # Re-raise exception if produced\n",
    "                for future in concurrent.futures.as_completed(futures):\n",
    "                    future.result()\n",
    "                executor.shutdown(wait=True)\n",
    "            else:\n",
    "                for runner in self.runners[batch]:\n",
    "                    runner.run(output_dir=base_output_dir)\n",
    "                    \n",
    "            \n",
    "    def PR_runners(self):\n",
    "        '''\n",
    "        Plot PR curves for each dataset\n",
    "        for all the algorithms\n",
    "        '''\n",
    "        for dataset in self.input_settings.datasets:              \n",
    "            PRCurves(dataset, self.input_settings)\n",
    "                \n",
    "                \n",
    "class ConfigParser(object):\n",
    "    '''\n",
    "    Define static methods for parsing a config file that sets a large number\n",
    "    of parameters for the pipeline\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def parse(config_file_handle) -> Evaluation:\n",
    "        config_map = yaml.load(config_file_handle)\n",
    "        return Evaluation(\n",
    "            ConfigParser.__parse_input_settings(\n",
    "                config_map['input_settings']),\n",
    "            ConfigParser.__parse_output_settings(\n",
    "                config_map['output_settings']))\n",
    "\n",
    "    @staticmethod\n",
    "    def __parse_input_settings(input_settings_map) -> InputSettings:\n",
    "        input_dir = input_settings_map['input_dir']\n",
    "        dataset_dir = input_settings_map['dataset_dir']\n",
    "        datasets = input_settings_map['datasets']\n",
    "\n",
    "        return InputSettings(\n",
    "                Path(input_dir, dataset_dir),\n",
    "                datasets,\n",
    "                ConfigParser.__parse_algorithms(\n",
    "                input_settings_map['algorithms']))\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def __parse_algorithms(algorithms_list):\n",
    "        algorithms = []\n",
    "        for algorithm in algorithms_list:\n",
    "                combos = [dict(zip(algorithm['params'], val))\n",
    "                    for val in itertools.product(\n",
    "                        *(algorithm['params'][param]\n",
    "                            for param in algorithm['params']))]\n",
    "                for combo in combos:\n",
    "                    algorithms.append([algorithm['name'],combo])\n",
    "            \n",
    "\n",
    "        return algorithms\n",
    "\n",
    "    @staticmethod\n",
    "    def __parse_output_settings(output_settings_map):\n",
    "        output_dir = Path(output_settings_map['output_dir'])\n",
    "        return OutputSettings(output_dir)\n",
    "\n",
    "def get_parser() -> argparse.ArgumentParser:\n",
    "    '''\n",
    "    :return: an argparse ArgumentParser object for parsing command\n",
    "        line parameters\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Run pathway reconstruction pipeline.')\n",
    "\n",
    "    parser.add_argument('--config', default='config.yaml',\n",
    "        help='Configuration file')\n",
    "\n",
    "    return parser\n",
    "\n",
    "def parse_arguments():\n",
    "    '''\n",
    "    Initialize a parser and use it to parse the command line arguments\n",
    "    :return: parsed dictionary of command line arguments\n",
    "    '''\n",
    "    parser = get_parser()\n",
    "    opts = parser.parse_args()\n",
    "\n",
    "    return opts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\", 'r') as conf:\n",
    "    evaluation = ConfigParser.parse(conf)\n",
    "print(evaluation)\n",
    "print('Evaluation started')\n",
    "\n",
    "# Do something\n",
    "\n",
    "print('Evaluation complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.input_settings.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SCODE', {'should_run': True, 'nGenes': 100, 'z': 4, 'nCells': 356, 'iIter': 100, 'nRep': 2}]\n",
      "['SCNS', {'should_run': False}]\n"
     ]
    }
   ],
   "source": [
    "for alg in evaluation.input_settings.algorithms:\n",
    "    print(alg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/adyprat/ModelEval/inputs/simulated')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path.cwd().joinpath(evaluation.input_settings.datadir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.runners[0].generateInputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#os.system(\"jupyter nbconvert --to script bench.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "parser_f() got an unexpected keyword argument 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d9caa2fe1b87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPR_runners\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-cf2ea8ffb62b>\u001b[0m in \u001b[0;36mPR_runners\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m         '''\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mPRCurves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ModelEval/src/plotPR.py\u001b[0m in \u001b[0;36mPRCurves\u001b[0;34m(dataDict, inputSettings)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     trueEdgesFile = pd.read_csv(str(inputSettings.datadir) + dataDict['trueEdges'],\n\u001b[0;32m----> 9\u001b[0;31m                                 sep = '\\t', header = None, index_col = None)\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: parser_f() got an unexpected keyword argument 'index'"
     ]
    }
   ],
   "source": [
    "evaluation.inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
